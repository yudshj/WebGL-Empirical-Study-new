{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:25:49.277149Z",
     "start_time": "2023-05-10T05:25:49.223004100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib_venn\n",
    "import matplotlib.pyplot as plt\n",
    "DATE='0527'\n",
    "raf_path = Path('../2-9.playwright/output/raf/')\n",
    "imr = Path('./imr')\n",
    "\n",
    "imr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def deal(path):\n",
    "    try:\n",
    "        with gzip.open(path, 'rt') as fp:\n",
    "            ret = json.load(fp)\n",
    "    except Exception as e:\n",
    "        print(path)\n",
    "        raise e\n",
    "    ret['raf_path'] = path.as_posix()\n",
    "    idx = path.as_posix().split('/')[-1].split('.')[0]\n",
    "    try:\n",
    "        ret['idx'] = int(idx)\n",
    "    except ValueError:\n",
    "        ret['idx'] = idx\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:12:05.053637900Z",
     "start_time": "2023-05-10T05:12:04.970478500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/0501-df_label-raf.pkl.zst.bak'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2974787/3837536719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 0413 + 0504_czy = 0504-df_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/0501-df_label-raf.pkl.zst.bak'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zstd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0mopen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cctx\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZstdCompressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m             handle = zstd.open(\n\u001b[0m\u001b[1;32m    841\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/zstandard/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, cctx, dctx, encoding, errors, newline, closefd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0minner_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_open_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mclosefd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/0501-df_label-raf.pkl.zst.bak'"
     ]
    }
   ],
   "source": [
    "# 0413 + 0504_czy = 0504-df_label\n",
    "df_label = pd.read_pickle('./output/0501-df_label-raf.pkl.zst.bak', compression='zstd')\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2974787/2188905058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_comments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_label' is not defined"
     ]
    }
   ],
   "source": [
    "df_label['label_comments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_ERROR  label_usecase  label_interactions  label_comments\n",
       "0.0          1              0                                     14098\n",
       "                            1                                       831\n",
       "                                                waiting             206\n",
       "                            0                   scroll              101\n",
       "             4              0                                        34\n",
       "             2              1                                        24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label2 = pd.read_pickle('./input/2023-05-09_0-30000_output.pkl').rename(columns={\n",
    "    'error': 'label_ERROR',\n",
    "    'comments': 'label_comments',\n",
    "    'use_case': 'label_usecase',\n",
    "    'need_interactions': 'label_interactions',\n",
    "})\n",
    "df_label2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2974787/1851965353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_comments'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label_ERROR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_label' is not defined"
     ]
    }
   ],
   "source": [
    "df_label.update(df_label2)\n",
    "df_label['category'] = df_label['category'].map(lambda x: x if type(x) == list else eval(x))\n",
    "df_label.loc[df_label['label_comments'] =='domain', 'label_ERROR'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def url_essential(url: str):\n",
    "    re_pattern = r'^(http(s)?://)?(.*?)(/?index(\\.[a-z]+)?)?[#/]*$'\n",
    "    return re.match(re_pattern, url).group(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2974787/2791747632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url_essential'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_essential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url_essential'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url_essential'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_ERROR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_label' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = df_label.copy()\n",
    "tmp['url_essential'] = df_label['url'].map(url_essential)\n",
    "tmp.drop_duplicates(subset=['url_essential'], inplace=True)\n",
    "tmp.drop(columns=['url_essential'], inplace=True)\n",
    "tmp['label_ERROR'].value_counts().drop(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('output/0518-df_label-raf-need_interactions.json', 'w') as fp:\n",
    "    json.dump([['{:05d}'.format(idx), x['url']] for idx, x in df_label[(df_label['label_interactions']==1)|(df_label['label_comments']=='TODO')].iterrows()], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_ERROR\n",
       "0    26034\n",
       "1     2161\n",
       "4      247\n",
       "3      187\n",
       "2       81\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label['label_ERROR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:12:05.218500600Z",
     "start_time": "2023-05-10T05:12:05.051637900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "bq              20906\n",
       "awwwards         4097\n",
       "tranco           3489\n",
       "cssdesign        1326\n",
       "threejs           150\n",
       "tfjs_gallery       82\n",
       "googleart          37\n",
       "custom             10\n",
       "david              10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp4 = tmp['category']\n",
    "tmp1 = tmp4.explode()\n",
    "tmp1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:20:15.877336100Z",
     "start_time": "2023-05-10T05:20:15.802333400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "[HTTP Archive]                        20779\n",
       "[Crawling/Galleries]                   4663\n",
       "[Crawling/Top list]                    3485\n",
       "[Crawling/Galleries, HTTP Archive]      123\n",
       "[]                                       37\n",
       "[Crawling/Top list, HTTP Archive]         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = {\n",
    "    'bq': 'HTTP Archive',\n",
    "    'awwwards': 'Crawling/Galleries',\n",
    "    'tranco': 'Crawling/Top list',\n",
    "    'cssdesign': 'Crawling/Galleries',\n",
    "    'threejs': 'Crawling/Galleries',\n",
    "    'tfjs_gallery': 'Crawling/Galleries',\n",
    "    'googleart': 'Crawling/Galleries',\n",
    "    'custom': 'Crawling/Galleries',\n",
    "    'david': 'Crawling/Galleries',\n",
    "}\n",
    "tmp2 = tmp4.map(lambda x: sorted(set([translate[y] for y in x])))\n",
    "tmp2.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE DATASOURCE COLLECTED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:24:34.703027300Z",
     "start_time": "2023-05-10T05:24:34.663310800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp2e = tmp2.explode()\n",
    "tmp2e.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2e = tmp[tmp['label_ERROR'] == 0]\n",
    "tmp2e['label_interactions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T05:25:56.657614900Z",
     "start_time": "2023-05-10T05:25:56.226940900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matplotlib_venn.venn3([set(tmp2e[tmp2e == 'Crawling/Galleries'].index), set(tmp2e[tmp2e == 'Crawling/Top list'].index), set(tmp2e[tmp2e == 'HTTP Archive'].index)], set_labels=['Crawling/Galleries', 'Crawling/Top list', 'HTTP Archive'])\n",
    "plt.savefig('fig/results_overview_coll_venn.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE='0527'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T03:38:31.389101236Z",
     "start_time": "2023-05-02T03:34:29.055599998Z"
    }
   },
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_raw_all-raf.pkl.zst').exists():\n",
    "    df = pd.read_pickle(f'imr/{DATE}-df_raw_all-raf.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df = pd.DataFrame([deal(p) for p in sorted(raf_path.glob(\"*.gz\"))]).set_index('idx')\n",
    "    df = df_label.drop(columns=['url']).join(df)\n",
    "    # df.drop(df['url'].map(url_essential).duplicated().index, inplace=True)\n",
    "    df['url_essential'] = df['url'].map(url_essential)\n",
    "    df.drop_duplicates(subset=['url_essential'], inplace=True)\n",
    "    df.to_pickle(f'imr/{DATE}-df_raw_all-raf.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(x):\n",
    "    try:\n",
    "        return any([y['value']['data']['usedWebGL'] for y in x])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[df['label_interactions'] == 1]\n",
    "tmp['Used WebGL'] = tmp['frame'].str['gl_raf_counters'].map(ttt)\n",
    "tmp.drop(columns=['frame','events_time_hp','raf_path']).to_csv('A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T03:39:06.909139405Z",
     "start_time": "2023-05-02T03:38:31.388909443Z"
    }
   },
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_frame-raf.pkl.zst').exists():\n",
    "    df_frame = pd.read_pickle(f'imr/{DATE}-df_frame-raf.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df.dropna(subset=['frame'], inplace=True)\n",
    "    df_frame = df.drop(columns=['frame', 'events_time_hp']).join(pd.DataFrame(df['frame'].to_list(), index=df.index)['gl_rafs'].explode()).reset_index(drop=False).rename(columns={'index': 'idx'})\n",
    "    df_frame = df_frame.drop(columns=['gl_rafs']).join(pd.DataFrame(df_frame['gl_rafs'].str['value'].to_list(), index=df_frame.index)['data'])\n",
    "    df_frame = df_frame.drop(columns=['data']).join(pd.json_normalize(df_frame['data']).add_prefix('frame.'), how='right').dropna(subset=['frame.counter.usedWebGL']) #.drop_duplicates(subset=['frame.url']).reset_index(drop=True)\n",
    "    df_frame.to_pickle(f'imr/{DATE}-df_frame-raf.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-02T03:39:06.908969396Z"
    }
   },
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_context-raf.pkl.zst').exists():\n",
    "    df_context = pd.read_pickle(f'imr/{DATE}-df_context-raf.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df_context = df_frame.explode('frame.contextInfo').reset_index(drop=False).rename(columns={'index': 'idx-frame'})\n",
    "    df_context = df_context.drop(columns=['frame.contextInfo']).join(\n",
    "        pd.json_normalize(df_context['frame.contextInfo']).add_prefix('frame.context.'))\n",
    "    df_context.to_pickle(f'imr/{DATE}-df_context-raf.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_context-noraf.pkl.zst').exists():\n",
    "    df_context_noraf = pd.read_pickle(f'imr/{DATE}-df_context-noraf.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df_context_noraf = df_context.drop(columns=['frame.context.rafList'])\n",
    "    df_context_noraf.to_pickle(f'imr/{DATE}-df_context-noraf.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_shader-noraf.pkl.zst').exists():\n",
    "    df_shader = pd.read_pickle(f'imr/{DATE}-df_shader-noraf.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df_shader = df_context_noraf[['idx-frame','idx','category','label_ERROR','label_usecase','label_interactions','label_comments','url','date','raf_path','frame.url','frame.title','frame.date','frame.when','frame.context.maghsk.shaders',]].explode('frame.context.maghsk.shaders').dropna(subset=['frame.context.maghsk.shaders']).reset_index(drop=False).rename(columns={'index': 'idx-context'})\n",
    "    df_shader = df_shader.drop(columns='frame.context.maghsk.shaders').join(pd.json_normalize(df_shader['frame.context.maghsk.shaders']).add_prefix('frame.context.maghsk.shaders.')).dropna(subset=['frame.context.maghsk.shaders.source'])\n",
    "    df_shader = df_shader.explode('frame.context.maghsk.shaders.source').dropna(subset=['frame.context.maghsk.shaders.source'])\n",
    "    df_shader['frame.context.maghsk.shaders.type'] = df_shader['frame.context.maghsk.shaders.type'].astype(str).replace('35632', 'frag').replace('35633', 'vert')\n",
    "    df_shader['frame.context.maghsk.shaders.length'] = df_shader['frame.context.maghsk.shaders.source'].map(lambda x: len(x))\n",
    "    df_shader.to_pickle(f'imr/{DATE}-df_shader-noraf.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shader.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_path = Path('../2-9.playwright/output/spector-tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spector_deal(p):\n",
    "    ret = json.loads(p.read_text())\n",
    "    ret['spec_path'] = p.as_posix()\n",
    "    frame = []\n",
    "    for spector in ret['spector']:\n",
    "        try:\n",
    "            frame.append({\"url\": spector['value']['url'], \"data\": [base64.b64decode(x[22:]) for x in spector['value']['data']]})\n",
    "        except:\n",
    "            pass\n",
    "    del ret['spector']\n",
    "    ret['frame'] = frame\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29242/29242 [00:45<00:00, 649.67it/s] \n"
     ]
    }
   ],
   "source": [
    "if Path(f'imr/{DATE}-df_spector.pkl.zst').exists():\n",
    "    df = pd.read_pickle(f'imr/{DATE}-df_spector.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df = pd.DataFrame([spector_deal(p) for p in tqdm.tqdm(sorted(spec_path.glob(\"*.json\")))]).set_index('idx')\n",
    "    df = df.explode('frame').dropna(subset=['frame']).reset_index(drop=False).rename(columns={'index': 'idx'})\n",
    "    df = df.drop(columns='frame').join(pd.json_normalize(df['frame']).add_prefix('frame.')).explode('frame.data').dropna(subset='frame.data').reset_index(drop=False).rename(columns={'index': 'idx-frame'})\n",
    "    df['frame.data'] = df['frame.data'].map(lambda x: json.loads(gzip.decompress(x)))\n",
    "    df.to_pickle(f'imr/{DATE}-df_spector.pkl.zst', compression='zstd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_path = Path('../2-9.playwright/output/simple/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_simple(p):\n",
    "    with gzip.open(p, 'rt') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_simple.pkl.zst').exists():\n",
    "    df = pd.read_pickle(f'imr/{DATE}-df_simple.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df_raw = pd.DataFrame([deal_simple(p) for p in tqdm.tqdm(sorted(simple_path.glob(\"*.gz\")))])\n",
    "    df = df_raw.drop(columns=['frame', 'events_time_hp']).join(pd.json_normalize(df_raw['frame']).drop(columns=['net_idle_counters','gl_simple_counters'])).explode('gl_simples').dropna(subset='gl_simples').reset_index(drop=False).rename(columns={'index': 'idx'})\n",
    "    df = df.drop(columns='gl_simples').join(pd.json_normalize(df['gl_simples'].str['value']).drop(columns=['data', 'error.name']).add_prefix('frame.')).explode('frame.data.contextInfo').dropna(subset=['frame.data.contextInfo']).reset_index(drop=False).rename(columns={'index': 'idx-frame'})\n",
    "    df = df.drop(columns=['frame.data.contextInfo']).join(pd.json_normalize(df['frame.data.contextInfo']).add_prefix('frame.data.contextInfo.'))\n",
    "    df.to_pickle(f'imr/{DATE}-df_simple.pkl.zst', compression='zstd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_worker(p):\n",
    "    with gzip.open(p, 'rt') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_path = Path('../2-9.playwright/output/worker/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f'imr/{DATE}-df_worker_oc.pkl.zst').exists() and Path(f'imr/{DATE}-df_frame_oc.pkl.zst').exists():\n",
    "    df_worker_oc = pd.read_pickle(f'imr/{DATE}-df_worker_oc.pkl.zst', compression='zstd')\n",
    "    df_frame_oc = pd.read_pickle(f'imr/{DATE}-df_frame_oc.pkl.zst', compression='zstd')\n",
    "else:\n",
    "    df_raw = pd.DataFrame([deal_worker(p) for p in sorted(worker_path.glob(\"*.gz\"))])\n",
    "    df = df_raw.drop(columns=['netIdleTimeout', 'events_time_hp'])\n",
    "    df_worker_oc = df.drop(columns='frameOffscreenCanvasTypes').explode('workerOffscreenCanvasTypes')\n",
    "    df_frame_oc = df.drop(columns='workerOffscreenCanvasTypes').explode('frameOffscreenCanvasTypes')\n",
    "    df_worker_oc['workerOffscreenCanvasTypes'] = df_worker_oc['workerOffscreenCanvasTypes'].str['value']\n",
    "    df_frame_oc['frameOffscreenCanvasTypes'] = df_frame_oc['frameOffscreenCanvasTypes'].str['value']\n",
    "    df_worker_oc.dropna(inplace=True)\n",
    "    df_frame_oc.dropna(inplace=True)\n",
    "    df_worker_oc = df_worker_oc.drop(columns='workerOffscreenCanvasTypes').join(pd.json_normalize(df_worker_oc['workerOffscreenCanvasTypes']).drop(columns=['data', 'error', 'error.name']).fillna(0).add_prefix('worker.')).reset_index(drop=True)\n",
    "    df_frame_oc = df_frame_oc.drop(columns='frameOffscreenCanvasTypes').join(pd.json_normalize(df_frame_oc['frameOffscreenCanvasTypes']).drop(columns=['data', 'error', 'error.name']).fillna(0).add_prefix('frame.')).reset_index(drop=True)\n",
    "    df_worker_oc['worker.data.sum'] = df_worker_oc[['worker.data.webgl', 'worker.data.webgl2', 'worker.data.2d']].sum(axis=1)\n",
    "    df_frame_oc['frame.data.sum'] = df_frame_oc[['frame.data.webgl', 'frame.data.webgl2', 'frame.data.2d']].sum(axis=1)\n",
    "    df_worker_oc = df_worker_oc[df_worker_oc['worker.data.sum'] > 0]\n",
    "    df_frame_oc = df_frame_oc[df_frame_oc['frame.data.sum'] > 0]\n",
    "    df_worker_oc.drop(columns='worker.data.sum', inplace=True)\n",
    "    df_frame_oc.drop(columns='frame.data.sum', inplace=True)\n",
    "    df_worker_oc.to_pickle(f'imr/{DATE}-df_worker_oc.pkl.zst', compression='zstd')\n",
    "    df_frame_oc.to_pickle(f'imr/{DATE}-df_frame_oc.pkl.zst', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
